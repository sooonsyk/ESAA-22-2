{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmUw5dcSm0i+brzb3eRGV3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sooonsyk/ESAA-22-2/blob/main/W4_HW_mon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **앙상블 학습과 랜덤 포레스트**\n",
        "- 무작위로 선택된 수천 명의 사람에게 복잡한 질문을 하고 대답을 모으는 것이 전문가의 답보다 나음 - 대중의 지혜\n",
        "- 일련의 예측기(앙상블)로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있음 - 앙상블 학습\n",
        "  - 결정 트리의 앙상블 - 랜덤 포레스트"
      ],
      "metadata": {
        "id": "hPfnqWQBOV_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7.1 투표 기반 분류기**\n",
        "- 각 분류기의 예측을 모아서 가장 많이 선택된 클래스를 예측하는 것, 다수결 투표로 정해짐\n",
        "- 각 분류기가 약한 학습기일지라도 충분하게 많고 다양하다면 앙상블은 강한 학습기가 될 수 있음\n",
        "  - 큰 수의 법칙 때문\n",
        "  - 모든 분류기가 완변하게 독립적이고 오차에 상관관계가 없어야 가능 - 같은 데이터로 훈련시키기 때문에 가정에 맞지 않음\n",
        "  - 분류기가 가능한한 서로 독립적일 대 최고의 성능 발휘 - 각기 다른 알고리즘으로 학습시키면 다양한 분류기 얻을 수 있음\n",
        "    - 다른 종류의 오차를 만들어서 정확도 향상 시킴"
      ],
      "metadata": {
        "id": "0Xq9-Cj3O2qS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "nMbj1L45TMPr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42pbHIs3ONL6",
        "outputId": "67a942fb-c3c8-4db7-bbee-547d45956c83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
              "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier # 사이킷런의 투표 기반 분류기\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting = 'hard'\n",
        ")\n",
        "voting_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-wDoKXFQXtV",
        "outputId": "c63e9254-2784-4474-da3d-799d74257bad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression 0.864\n",
            "RandomForestClassifier 0.904\n",
            "SVC 0.896\n",
            "VotingClassifier 0.912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 투표 기반 분류기가 성능 더 높음\n",
        "- **간접 투표** :  모든 분류기가 클래스의 확률을 예측할 수 있으면(predict_proba()메서드가 있으면) 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측 \n",
        "  - 확률이 높은 투표에 비중을 두기 때문에 직접 투표 방식보다 성능이 높음\n",
        "  - voting = \"soft\"로 바꾸고 모든 분류기가 클래스의 확률을 추정할 수 있으면 됨"
      ],
      "metadata": {
        "id": "yEaQEur2TvlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.2 배깅과 페이스팅**\n",
        "- 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 분류기를 각각 다르게 학습시키는 것\n",
        "- **배깅** : 훈련 세트에서 중복을 허용하여 샘플링하는 방식\n",
        "- **페이스팅** : 중복을 허용하지 않고 샘플링하는 방식\n",
        "- 모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아서 새로운 샘플에 대한 예측 만듦\n",
        "- 수집 함수는 전형적으로 분류일 때는 통계적 최빈값, 회귀에 대해서는 평균을 계산\n",
        "- 개별 예측기는 원본 훈련 세트로 훈련시킨 것보다 훨씬 크게 편향되어 있지만 수집 함수를 통과하면 편향과 분산이 모두 감소\n",
        "- 일반적으로 앙상블의 결과는 원본 데이터셋으로 하나의 예측기를 훈련시킬 때와 비교해 편향은 비슷하지만 분산은 줄어듦"
      ],
      "metadata": {
        "id": "YzoglonmUKAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**사이킷런의 배깅과 페이스팅**"
      ],
      "metadata": {
        "id": "RDp8RquYVsA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500, #결정트리 분류기 500개의 앙상블\n",
        "    max_samples = 100, bootstrap=True, #각 분류기는 훈련세트에서 중복을 허용하여 무작위로 선택된 100개의 샘플로 훈련 \n",
        "    n_jobs=1 #CPU 코어수 지정, -1은 모든 코어 사용\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "cCGE-qxeTpPB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BaggingClassifier는 클래스 확률을 추정할 수 있으면 직접 투표 대신 간접 투표 방식 사용"
      ],
      "metadata": {
        "id": "4bUqndRsXP-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**7.2.2 oob 평가**\n",
        "- 배깅을 사용하면 어떤 샘플은 한 예측기를 위해 여러 번 샘플링 되고 어떤 것은 전혀 선택되지 않을 수 있음 - 선택되지 않은 훈련 샘플을 oob 샘플이라고 부름, 예측기마다 다름\n",
        "- 예측기가 훈련되는 동안에는 oob샘픔을 사용하지 않으므로 별도의 검증 세트를 사용하지 않고 oob 샘플 사용해 평가할 수 잇음"
      ],
      "metadata": {
        "id": "GkrnLp2JXlSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    bootstrap=True, n_jobs=-1, oob_score=True) #oob_score=True 로 지정하면 훈련 끝난 후 자동으로 oob 평가 수행\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQSNeEfHXLzE",
        "outputId": "ba91d8ca-779a-4633-8c83-94908ce45044"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9013333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwZ5rjjeYZh3",
        "outputId": "ee942fa8-d1bb-4c08-f445-6ba390be4141"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.896"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf.oob_decision_function_\n",
        "#oob 샘플에 대한 결정 함수 값 - 이 경우 각 훈련 샘플의 클래스 확률"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK3qzWiOZGHb",
        "outputId": "4b53cea5-988e-4c6d-e052-ffb74f55bd7c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.40437158, 0.59562842],\n",
              "       [0.35675676, 0.64324324],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.09230769, 0.90769231],\n",
              "       [0.36363636, 0.63636364],\n",
              "       [0.01156069, 0.98843931],\n",
              "       [0.96067416, 0.03932584],\n",
              "       [0.95238095, 0.04761905],\n",
              "       [0.76923077, 0.23076923],\n",
              "       [0.00564972, 0.99435028],\n",
              "       [0.78074866, 0.21925134],\n",
              "       [0.83957219, 0.16042781],\n",
              "       [0.97237569, 0.02762431],\n",
              "       [0.07692308, 0.92307692],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98958333, 0.01041667],\n",
              "       [0.94623656, 0.05376344],\n",
              "       [1.        , 0.        ],\n",
              "       [0.0212766 , 0.9787234 ],\n",
              "       [0.41317365, 0.58682635],\n",
              "       [0.91176471, 0.08823529],\n",
              "       [1.        , 0.        ],\n",
              "       [0.9787234 , 0.0212766 ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.61797753, 0.38202247],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00534759, 0.99465241],\n",
              "       [0.09677419, 0.90322581],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.32291667, 0.67708333],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.18324607, 0.81675393],\n",
              "       [0.38202247, 0.61797753],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.02941176, 0.97058824],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01546392, 0.98453608],\n",
              "       [0.99450549, 0.00549451],\n",
              "       [0.88108108, 0.11891892],\n",
              "       [0.98245614, 0.01754386],\n",
              "       [0.9760479 , 0.0239521 ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.04046243, 0.95953757],\n",
              "       [0.9947644 , 0.0052356 ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99431818, 0.00568182],\n",
              "       [0.78453039, 0.21546961],\n",
              "       [0.41798942, 0.58201058],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.640625  , 0.359375  ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.8241206 , 0.1758794 ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.57324841, 0.42675159],\n",
              "       [0.11764706, 0.88235294],\n",
              "       [0.63157895, 0.36842105],\n",
              "       [0.91836735, 0.08163265],\n",
              "       [0.        , 1.        ],\n",
              "       [0.17857143, 0.82142857],\n",
              "       [0.88541667, 0.11458333],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02234637, 0.97765363],\n",
              "       [0.05780347, 0.94219653],\n",
              "       [0.34146341, 0.65853659],\n",
              "       [1.        , 0.        ],\n",
              "       [0.0052356 , 0.9947644 ],\n",
              "       [0.85185185, 0.14814815],\n",
              "       [0.0049505 , 0.9950495 ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.21666667, 0.78333333],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.91011236, 0.08988764],\n",
              "       [0.77777778, 0.22222222],\n",
              "       [0.00531915, 0.99468085],\n",
              "       [1.        , 0.        ],\n",
              "       [0.19662921, 0.80337079],\n",
              "       [0.56593407, 0.43406593],\n",
              "       [0.        , 1.        ],\n",
              "       [0.05405405, 0.94594595],\n",
              "       [0.44607843, 0.55392157],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00526316, 0.99473684],\n",
              "       [0.99487179, 0.00512821],\n",
              "       [0.24747475, 0.75252525],\n",
              "       [0.55319149, 0.44680851],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01104972, 0.98895028],\n",
              "       [0.99401198, 0.00598802],\n",
              "       [0.32663317, 0.67336683],\n",
              "       [0.89830508, 0.10169492],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.7688172 , 0.2311828 ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00571429, 0.99428571],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.98342541, 0.01657459],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.97354497, 0.02645503],\n",
              "       [1.        , 0.        ],\n",
              "       [0.02094241, 0.97905759],\n",
              "       [0.24479167, 0.75520833],\n",
              "       [0.97297297, 0.02702703],\n",
              "       [0.26744186, 0.73255814],\n",
              "       [0.98930481, 0.01069519],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00526316, 0.99473684],\n",
              "       [0.73142857, 0.26857143],\n",
              "       [0.41860465, 0.58139535],\n",
              "       [0.40201005, 0.59798995],\n",
              "       [0.88421053, 0.11578947],\n",
              "       [0.91954023, 0.08045977],\n",
              "       [0.05882353, 0.94117647],\n",
              "       [0.76963351, 0.23036649],\n",
              "       [0.01030928, 0.98969072],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02209945, 0.97790055],\n",
              "       [0.9744898 , 0.0255102 ],\n",
              "       [0.99453552, 0.00546448],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02717391, 0.97282609],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.96808511, 0.03191489],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.99444444, 0.00555556],\n",
              "       [0.        , 1.        ],\n",
              "       [0.41981132, 0.58018868],\n",
              "       [0.25568182, 0.74431818],\n",
              "       [0.01704545, 0.98295455],\n",
              "       [0.        , 1.        ],\n",
              "       [0.30994152, 0.69005848],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00543478, 0.99456522],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98324022, 0.01675978],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01117318, 0.98882682],\n",
              "       [0.7183908 , 0.2816092 ],\n",
              "       [0.92307692, 0.07692308],\n",
              "       [0.        , 1.        ],\n",
              "       [0.98924731, 0.01075269],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.07692308, 0.92307692],\n",
              "       [1.        , 0.        ],\n",
              "       [0.04301075, 0.95698925],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02150538, 0.97849462],\n",
              "       [0.99453552, 0.00546448],\n",
              "       [0.92708333, 0.07291667],\n",
              "       [0.7431694 , 0.2568306 ],\n",
              "       [0.5923913 , 0.4076087 ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.18823529, 0.81176471],\n",
              "       [1.        , 0.        ],\n",
              "       [0.95833333, 0.04166667],\n",
              "       [0.96531792, 0.03468208],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00568182, 0.99431818],\n",
              "       [0.        , 1.        ],\n",
              "       [0.41818182, 0.58181818],\n",
              "       [0.82777778, 0.17222222],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01530612, 0.98469388],\n",
              "       [0.        , 1.        ],\n",
              "       [0.96296296, 0.03703704],\n",
              "       [0.00510204, 0.99489796],\n",
              "       [0.21578947, 0.78421053],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.97191011, 0.02808989],\n",
              "       [0.81683168, 0.18316832],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.07142857, 0.92857143],\n",
              "       [1.        , 0.        ],\n",
              "       [0.01570681, 0.98429319],\n",
              "       [0.        , 1.        ],\n",
              "       [0.05181347, 0.94818653],\n",
              "       [1.        , 0.        ],\n",
              "       [0.70454545, 0.29545455],\n",
              "       [0.        , 1.        ],\n",
              "       [0.84864865, 0.15135135],\n",
              "       [0.98979592, 0.01020408],\n",
              "       [0.17874396, 0.82125604],\n",
              "       [0.23163842, 0.76836158],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.28125   , 0.71875   ],\n",
              "       [0.97340426, 0.02659574],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.51980198, 0.48019802],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.10185185, 0.89814815],\n",
              "       [0.15306122, 0.84693878],\n",
              "       [0.96296296, 0.03703704],\n",
              "       [0.02673797, 0.97326203],\n",
              "       [1.        , 0.        ],\n",
              "       [0.38505747, 0.61494253],\n",
              "       [0.14450867, 0.85549133],\n",
              "       [0.51832461, 0.48167539],\n",
              "       [0.61403509, 0.38596491],\n",
              "       [0.00465116, 0.99534884],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.57377049, 0.42622951],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.22543353, 0.77456647],\n",
              "       [0.7804878 , 0.2195122 ],\n",
              "       [0.12777778, 0.87222222],\n",
              "       [1.        , 0.        ],\n",
              "       [0.82564103, 0.17435897],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.11229947, 0.88770053],\n",
              "       [0.03076923, 0.96923077],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.9       , 0.1       ],\n",
              "       [0.16201117, 0.83798883],\n",
              "       [0.98477157, 0.01522843],\n",
              "       [0.00520833, 0.99479167],\n",
              "       [0.61458333, 0.38541667],\n",
              "       [0.06532663, 0.93467337],\n",
              "       [1.        , 0.        ],\n",
              "       [0.78142077, 0.21857923],\n",
              "       [0.        , 1.        ],\n",
              "       [0.99447514, 0.00552486],\n",
              "       [0.94329897, 0.05670103],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00537634, 0.99462366],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.20670391, 0.79329609],\n",
              "       [0.99489796, 0.00510204],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.00505051, 0.99494949],\n",
              "       [0.83240223, 0.16759777],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.79005525, 0.20994475],\n",
              "       [0.93846154, 0.06153846],\n",
              "       [1.        , 0.        ],\n",
              "       [0.75842697, 0.24157303],\n",
              "       [0.4973545 , 0.5026455 ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.92682927, 0.07317073],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.85164835, 0.14835165],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.68681319, 0.31318681],\n",
              "       [0.11666667, 0.88333333],\n",
              "       [0.47222222, 0.52777778],\n",
              "       [0.2259887 , 0.7740113 ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.87356322, 0.12643678],\n",
              "       [0.83574879, 0.16425121],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.03141361, 0.96858639],\n",
              "       [0.98351648, 0.01648352],\n",
              "       [0.97126437, 0.02873563],\n",
              "       [1.        , 0.        ],\n",
              "       [0.57512953, 0.42487047],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00606061, 0.99393939],\n",
              "       [0.98113208, 0.01886792],\n",
              "       [0.02094241, 0.97905759],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.97461929, 0.02538071],\n",
              "       [0.        , 1.        ],\n",
              "       [0.03977273, 0.96022727],\n",
              "       [0.00561798, 0.99438202],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.03804348, 0.96195652],\n",
              "       [1.        , 0.        ],\n",
              "       [0.13114754, 0.86885246],\n",
              "       [0.        , 1.        ],\n",
              "       [0.02659574, 0.97340426],\n",
              "       [0.        , 1.        ],\n",
              "       [0.34848485, 0.65151515],\n",
              "       [0.06358382, 0.93641618],\n",
              "       [0.23595506, 0.76404494],\n",
              "       [1.        , 0.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.19047619, 0.80952381],\n",
              "       [0.98265896, 0.01734104],\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.94594595, 0.05405405],\n",
              "       [0.30851064, 0.69148936],\n",
              "       [0.97790055, 0.02209945],\n",
              "       [1.        , 0.        ],\n",
              "       [0.00526316, 0.99473684],\n",
              "       [0.98924731, 0.01075269],\n",
              "       [0.01020408, 0.98979592],\n",
              "       [0.05454545, 0.94545455],\n",
              "       [0.9895288 , 0.0104712 ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.05555556, 0.94444444],\n",
              "       [0.69892473, 0.30107527]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7.3 랜덤 패치와 랜덤 서브스페이스**\n",
        "- BaggingClassifier는 특성 샘플링 지원 - max_features, bootstrap_features 로 조절\n",
        "- **랜덤 패치 방식** : 훈련 특성과 샘플을 모두 샘플링하는 것\n",
        "- **랜덤 서브스페이스 방식** : 훈련 샘플을 모두 사용하고 특성은 샘플링하는 것\n",
        "  - bootstrap=False, max_samples=1.0, bootstrap_features=True, max_features 1.0보다 작게 설정\n",
        "  - 특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리는 대신 분산을 낮춤\n",
        "  "
      ],
      "metadata": {
        "id": "ujKG4KLzZZSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7.4 랜덤 포레스트**\n",
        "- 일반적으로 배깅 또는 페이스팅을 적용한 결정 트리 앙상블, 전형적으로 max_samples를 훈련 세트의 크기로 지정\n",
        "- BaggingClassifier에 DecisionTreeClassifier를 넣어 만드는 대신 결정 트리에 최적화되어 사용하기 편한 DecisionTreeClassifier 사용 가능"
      ],
      "metadata": {
        "id": "dDUMQtz8aOO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "RjjhRd2GZWSm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 랜덤 포레스트는 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 무작위성을 더 주입함 - 트리 더 다양하게 만들고 편향을 손해보는 대신 분산을 낮추어 더 훌륭한 모델 만들어냄"
      ],
      "metadata": {
        "id": "BrCDkkiQa-pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**7.4.1 엑스트라 트리**\n",
        "- **익스트림 랜덤 트리** : 극단적으로 무작위한 트리의 랜덤 포레스트\n",
        "  - 모든 노드에서 특성마다 가장 최적의 임곗값을 찾는 것이 트리 알고리즘에서 가장 시간이 많이 소요되는 작업 중 하나이므로 일반적인 랜덤 포레스트보다 엑스트라 트리가 훨씬 빠름\n",
        "  - 사이킷런의 ExtraTreesClassifier를 사용"
      ],
      "metadata": {
        "id": "lwbbkmrmbSJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**7.4.2 특성 중요도**\n",
        "- 랜덤 포레스트는 특성의 상대적 중요도 측정하기 쉬움\n",
        "  - 어떤 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 측정\n",
        "  - 훈련이 끝난ㄴ 뒤 특성마다 자동으로 이 점수를 계산하고 중요도의 전체 합이 1이 되도록 결괏값 정규화함 - feature_importances_"
      ],
      "metadata": {
        "id": "dcLOX_W6b3iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
        "rnd_clf.fit(iris['data'], iris['target'])\n",
        "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
        "  print(name, score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO2pvEdta10I",
        "outputId": "d82a4ccc-6066-45e3-9726-812b59f707f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm) 0.11941625313301481\n",
            "sepal width (cm) 0.020420019411930737\n",
            "petal length (cm) 0.42380756675362147\n",
            "petal width (cm) 0.436356160701433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7.5 부스팅**\n",
        "- 약한 학습기를 여러개 연결하여 강한 학습기를 만드는 앙상블 방법\n",
        "- 앞의 모델을 보완해나가면서 일련의 예측기를 학습시키는 것"
      ],
      "metadata": {
        "id": "Dk1ytsztcuK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**7.5.1 에이다부스트**\n",
        "- 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이는 것 - 학습하기 어려운 샘플에 점점 더 맞춰지게 됨\n",
        "- 각 예측기는 이전 예측기가 훈련되고 평가된 후에 학습될 수 있기 때문에 병렬화할 수 없음\n",
        "- 새 예측기의 가중치가 계산되고 샘픔의 가중치를 업데이트 해서 또 다른 예측기를 훈련 시키는 식으로 반복\n",
        "- 지정된 예측기 수에 도달하거나 완변한 예측기가 만들어지면 중지됨\n",
        "- 예측할 때 단순히 모든 예측기의 예측을 게산하고 예측기 가중치를 더해 예측 결과를 만듦, 가중치 합이 가장 큰 클래스가 예측 결과가 됨\n",
        "- 사이킷런은 SAMME 라는 에이다부스트의 다중 클래스 버전을 사용 - 클래스가 두 개일 때는 에이다부스트와 동일\n",
        "  - 예측기가 클래스의 확률을 추정할 수 있다면 SAMME.R 사용"
      ],
      "metadata": {
        "id": "dOhO11gqc6ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200, #200개의 아주 얕은 결정 트리\n",
        "    algorithm='SAMME.R', learning_rate=0.5\n",
        ")\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "#에이다부스트의 기본 추정기\n",
        "#과대적합시 추정기 수를 줄이거나 규제를 강하게"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBoEdst_cnvS",
        "outputId": "5c03f2ff-192e-40ab-c6cf-131c9c8676ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
              "                   learning_rate=0.5, n_estimators=200)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**7.5.2 그레이디언트 부스팅**\n",
        "- 이전까지의 오차를 보정하도록 순차적으로 추가하지만 에이다부스트처럼 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습시킴"
      ],
      "metadata": {
        "id": "pPtYFJkjfk6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#잡음이 섞인 2차 곡선 형태의 훈련 세트\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) - 0.5\n",
        "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
      ],
      "metadata": {
        "id": "n6HCdvC_hVIY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg1.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja3kKzw-fah1",
        "outputId": "40630062-9fee-4448-f1c9-a090e56a8732"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#첫번째 예측기에서 생긴 잔여 오차에 두번째 훈련\n",
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg2.fit(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXeSe313gMkD",
        "outputId": "a76fc92e-98c4-4572-c30d-527ab64caa4e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#두번째 예측기가 만든 잔여 오차에 세번째 회귀 모델 훈련\n",
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg3.fit(X, y3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D29yRYSgYSy",
        "outputId": "4fcf490a-0063-4cd3-96dc-c61aa1fde8d4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(max_depth=2)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_new = np.array([[0.8]])"
      ],
      "metadata": {
        "id": "aGWvtbgXhIRu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3개의 트리를 포함하는 앙상블 모델\n",
        "#새로운 샘플에 대한 예측 만들려면 모든 트리의 예측 더함\n",
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ],
      "metadata": {
        "id": "JCY0MtDzgqN6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 사이킷런의 GreadientBoostingRegressor를 사용하면 간단하게 같은 앙상블 만들 수 있음"
      ],
      "metadata": {
        "id": "h28-CUP5h3zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "grbt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0) \n",
        "grbt.fit(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6V-xfjLg47a",
        "outputId": "1aa263fe-f086-496b-a0f8-f564a2b2cfa3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- learning_rate가 각 트리의 기여 정도 조절 - 낮게 설정하면 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만 예측의 성능은 좋아짐 - 축소 기법\n",
        "- 너무 많으면 과대적합될 수 있음\n",
        "- 최적의 트리 수 찾기 위해서 조기 종료 기법 사용 - staged_predict() 메서드 사용\n",
        "  - 훈련의 각 단게에서 앙상블에 의해 만들어진 예측기를 순회하는 반복자 반환"
      ],
      "metadata": {
        "id": "QJ4bhnCNiV_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors) + 1\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
        "gbrt_best.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmgs2FSMiPcD",
        "outputId": "5ffbd86c-9b37-4290-c954-542e17a6c6c9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(max_depth=2, n_estimators=85)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 많은 수의 트리를 먼저 훈련시키고 최적의 수를 찾기 위해 살펴보는 대신 실제로 훈련을 중지하는 방법으로 조기 종료 구현할 수 있음\n",
        "  - warm_start=True로 설정하면 사이킷런이 fit() 메서드가 호출될 때 기존 트리를 유지하고 훈련을 추가할 수 있도록 함"
      ],
      "metadata": {
        "id": "tQylvEzBje7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#연속해서 5번의 반복동안 검증 오차가 향상되지 않으면 훈련 멈춤\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
        "\n",
        "min_val_error = float('inf')\n",
        "error_going_up=0\n",
        "for n_estimators in range(1, 120):\n",
        "  gbrt.n_estimators = n_estimators\n",
        "  gbrt.fit(X_train, y_train)\n",
        "  y_pred = gbrt.predict(X_val)\n",
        "  val_error = mean_squared_error(y_val, y_pred)\n",
        "  if val_error < min_val_error:\n",
        "    min_val_error = val_error\n",
        "    error_going_up=0\n",
        "  else:\n",
        "    error_going_up += 1\n",
        "    if error_going_up == 5:\n",
        "      break #조기 종료"
      ],
      "metadata": {
        "id": "mLlKQWfDja0G"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 각 트리가 훈련할 때 사용할 훈련 샘플의 비율을 지정할 수 있는 subsample 매개변수 지원 - 편향이 높아지는 대신 분산 낮아짐, 훈련 속도 상당히 높임 - 확률적 그레이디언트 부스팅\n",
        "- 최적화된 그레이디언트 부스팅 구현으로 XGBoost - 익스트림 그레이디언트 부스팅"
      ],
      "metadata": {
        "id": "LBjn1KR3k_mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor()\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xXALvtek_Dw",
        "outputId": "67ccfc4a-55a8-4c76-f670-29ced51a2042"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13:00:50] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2) #조기 종료 기능 제공\n",
        "y_pred = xgb_reg.predict(X_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCBZtCg8lhVx",
        "outputId": "47bb8b57-62d7-4944-e34c-1f870cd030cf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13:01:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[0]\tvalidation_0-rmse:0.275313\n",
            "Will train until validation_0-rmse hasn't improved in 2 rounds.\n",
            "[1]\tvalidation_0-rmse:0.247499\n",
            "[2]\tvalidation_0-rmse:0.222947\n",
            "[3]\tvalidation_0-rmse:0.201049\n",
            "[4]\tvalidation_0-rmse:0.181641\n",
            "[5]\tvalidation_0-rmse:0.166735\n",
            "[6]\tvalidation_0-rmse:0.153621\n",
            "[7]\tvalidation_0-rmse:0.141686\n",
            "[8]\tvalidation_0-rmse:0.131314\n",
            "[9]\tvalidation_0-rmse:0.122047\n",
            "[10]\tvalidation_0-rmse:0.112234\n",
            "[11]\tvalidation_0-rmse:0.10366\n",
            "[12]\tvalidation_0-rmse:0.097232\n",
            "[13]\tvalidation_0-rmse:0.092018\n",
            "[14]\tvalidation_0-rmse:0.087123\n",
            "[15]\tvalidation_0-rmse:0.083105\n",
            "[16]\tvalidation_0-rmse:0.079681\n",
            "[17]\tvalidation_0-rmse:0.076621\n",
            "[18]\tvalidation_0-rmse:0.074141\n",
            "[19]\tvalidation_0-rmse:0.071896\n",
            "[20]\tvalidation_0-rmse:0.070115\n",
            "[21]\tvalidation_0-rmse:0.068281\n",
            "[22]\tvalidation_0-rmse:0.066894\n",
            "[23]\tvalidation_0-rmse:0.065783\n",
            "[24]\tvalidation_0-rmse:0.064704\n",
            "[25]\tvalidation_0-rmse:0.063744\n",
            "[26]\tvalidation_0-rmse:0.062625\n",
            "[27]\tvalidation_0-rmse:0.061978\n",
            "[28]\tvalidation_0-rmse:0.060858\n",
            "[29]\tvalidation_0-rmse:0.059868\n",
            "[30]\tvalidation_0-rmse:0.059202\n",
            "[31]\tvalidation_0-rmse:0.058817\n",
            "[32]\tvalidation_0-rmse:0.058233\n",
            "[33]\tvalidation_0-rmse:0.057706\n",
            "[34]\tvalidation_0-rmse:0.057434\n",
            "[35]\tvalidation_0-rmse:0.056745\n",
            "[36]\tvalidation_0-rmse:0.056413\n",
            "[37]\tvalidation_0-rmse:0.055872\n",
            "[38]\tvalidation_0-rmse:0.055391\n",
            "[39]\tvalidation_0-rmse:0.05512\n",
            "[40]\tvalidation_0-rmse:0.054845\n",
            "[41]\tvalidation_0-rmse:0.054576\n",
            "[42]\tvalidation_0-rmse:0.054318\n",
            "[43]\tvalidation_0-rmse:0.054347\n",
            "[44]\tvalidation_0-rmse:0.054104\n",
            "[45]\tvalidation_0-rmse:0.054106\n",
            "[46]\tvalidation_0-rmse:0.054133\n",
            "Stopping. Best iteration:\n",
            "[44]\tvalidation_0-rmse:0.054104\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7.6 스태킹**\n",
        "- 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수를 사용하는 대신 취합하는 모델을 훈련시킬 수 없을까?\n",
        "- 마지막 예측기가 최종 예측 만듦 - 블렌더 혹은 메타 학습기\n",
        "- 블렌더를 학습시키는 일반적인 방법은 홀드아웃 세트를 이용하는 것\n",
        "  - 훈련 세트를 두 개의 서브셋으로 나눔\n",
        "  - 첫번째 서브셋은 첫번째 레이어의 예측을 훈련시키기 위해 사용\n",
        "  - 첫번째 레이어의 예측기를 사용해 두번째 (홀드아웃) 세트에 대한 예측 만듦\n",
        "      - 예측기들이 훈련하는 동안 이 샘플들을 전혀 보지 못했기 때문에 이때 만들어진 예측은 완전히 새로운 것\n",
        "  - 홀드아웃세트의 각 샘플에 대해 예측기 계수만큼 예측값이 있음\n",
        "  - 타깃값은 그대로 쓰고 앞에서 예측한 값을 입력 특성으로 사용하는 새로운 훈련 세트 만듦 - 예측기 개수만큼의 차원 가짐\n",
        "  - 블랜더가 새 훈련 세트로 훈련됨 - 첫번재 레이어의 예측을 가지고 타깃값을 예측하도록 학습됨\n",
        "- 블렌더 여러 개 훈련 시키는 것도 가능 - 블렌더만의 레이어가 만들어짐\n",
        "- 사이킷런은 스태킹 직접 지원하지는 않음\n"
      ],
      "metadata": {
        "id": "ABl97J3KlzLe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWJW6Fsslu6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}